{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.extend([\"../src\", \"..\"])\n",
    "import sensai\n",
    "import logging\n",
    "import config\n",
    "\n",
    "cfg = config.get_config(reload=True)\n",
    "sensai.util.logging.configureLogging(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Evaluating Clustering Algorithms\n",
    "\n",
    "The present library contains utilities for evaluating different clustering algorithms\n",
    "(with or without ground truth labels). On top of the evaluation utilities there are classes for\n",
    "performing parameters sweeps and model selection. Here we give an overview of the most important functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint\n",
    "from sklearn.cluster import DBSCAN\n",
    "import seaborn as sns\n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "from sensai.geoanalytics.geopandas.coordinate_clustering import SkLearnCoordinateClustering\n",
    "from sensai.hyperopt import GridSearch\n",
    "from sensai.evaluation.evaluator_clustering import ClusteringModelSupervisedEvaluator, \\\n",
    "    ClusteringModelUnsupervisedEvaluator\n",
    "from sensai.evaluation.eval_stats import ClusteringUnsupervisedEvalStats, ClusteringSupervisedEvalStats, \\\n",
    "    AdjustedMutualInfoScore\n",
    "from sensai.geoanalytics.geopandas.coordinate_clustering_ground_truth import PolygonAnnotatedCoordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loading data \n",
    "sampleFile = cfg.datafile_path(\"sample\", stage=cfg.RAW) # this can point to a directory or a shp/geojson file\n",
    "coordinatesDF = gp.read_file(sampleFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluating a Single Model\n",
    "\n",
    "For a single model that was already fitted, evaluation statistics can be extracted with `ClusteringEvalStats`, see the\n",
    "example below (the eval_stats object can also be used to retrieve evaluation results one by one)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dbscan = SkLearnCoordinateClustering(DBSCAN(eps=150, min_samples=20))\n",
    "dbscan.fit(coordinatesDF)\n",
    "\n",
    "evalStats = ClusteringUnsupervisedEvalStats.fromModel(dbscan)\n",
    "\n",
    "pprint(evalStats.getAll())\n",
    "\n",
    "plt.hist(evalStats.clusterSizeDistribution)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Unsupervised Model Selection\n",
    "\n",
    "For model selection we need to compare different (or differently parametrized) models that were\n",
    "trained on the same dataset. The `ClusteringEvaluator` abstraction was designed with this goal in mind.\n",
    "The evaluator can be used to obtain evaluation statistics for different models that are guaranteed\n",
    "to be comparable with each other (always computed by the same object in the same way). Here is an example evaluating\n",
    "DBSCAN's performance on metrics that don't necessitate ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "modelEvaluator = ClusteringModelUnsupervisedEvaluator(coordinatesDF)\n",
    "\n",
    "dbscanEvalStats = modelEvaluator.evalModel(dbscan, fit=False)  # dbscan was already fitted on this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"dbscan_performance: \\n\")\n",
    "pprint(dbscanEvalStats.getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "One of the main purposes of evaluators is to be used within classes that perform a parameter sweep, e.g.\n",
    "a `GridSearch`. All such objects return a data frame and (optionally) persist all evaluation results\n",
    "in a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "parameterOptions = {\n",
    "    \"min_samples\": [10, 20],\n",
    "    \"eps\": [50, 150]\n",
    "}\n",
    "\n",
    "# for running the grid search in multiple processes, all objects need to be picklable.\n",
    "# Therefore we pass a named function as model factory instead of a lambda\n",
    "def dbscanFactory(**kwargs):\n",
    "    return SkLearnCoordinateClustering(DBSCAN(**kwargs))\n",
    "\n",
    "dbscanGridSearch = GridSearch(dbscanFactory, parameterOptions, csvResultsPath=os.path.join(cfg.temp, \"dbscanGridSearchCsv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# the results of the grid-search are saved as a CSV file under the path provided above\n",
    "resultDf = dbscanGridSearch.run(modelEvaluator, sortColumnName=\"numClusters\", ascending=False)\n",
    "resultDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The resulting data frame can be used to visualize the results through standard techniques,\n",
    "e.g. pivoting and heatmaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"calinskiHarabaszScores\")\n",
    "chScoreHeatmap = resultDf.pivot(index=\"min_samples\", columns=\"eps\", values=\"CalinskiHarabaszScore\")\n",
    "sns.heatmap(chScoreHeatmap, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"daviesBouldinScores\")\n",
    "chScoreHeatmap = resultDf.pivot(index=\"min_samples\", columns=\"eps\", values=\"DaviesBouldinScore\")\n",
    "sns.heatmap(chScoreHeatmap, cmap=sns.cm.rocket_r, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"numClusters\")\n",
    "numClustersHeatmap = resultDf.pivot(index=\"min_samples\", columns=\"eps\", values=\"numClusters\").astype(int)\n",
    "sns.heatmap(numClustersHeatmap, annot=True)  # something goes wrong with the datatype here, maybe b/c of zero clusters\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Supervised Model Selection\n",
    "\n",
    "### Obtaining Ground Truth Labels\n",
    "\n",
    "\n",
    "The evaluation classes can take ground truth labels for all coordinates and use them for calculating related metrics.\n",
    "However, such labels are typically hard to come by, especially if the coordinates cover a large area. Therefore the\n",
    "library includes utilities for extracting labels from ground truth provided in form of __cluster polygons in a selected\n",
    "region__. The central class for dealing with this kind of data is `ground_truth.PolygonAnnotatedCoordinates`,\n",
    "see examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The polygons can be read directly from a file, see the documentation for more details\n",
    "groundTruthClusters = PolygonAnnotatedCoordinates(coordinatesDF, cfg.datafile_path(\"sample\", stage=cfg.GROUND_TRUTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As usual, the object has methods for plotting and exporting to geodata frames.\n",
    "These can be very useful for inspecting the provided data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "groundTruthClusters.plot(markersize=0.2, cmap=\"plasma\")\n",
    "plt.show()\n",
    "\n",
    "groundTruthClusters.toGeoDF().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can extract the coordinates and labels for the annotated region and use them in evaluation. In the following,\n",
    "we will evaluate a slight adaptation of DBSCAN which uses an additional bound, i.e. it will ultimately reject clusters that do not reach a minimum size.\n",
    "We will train it on datapoints in the ground truth region and evaluate the results against the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "boundedDbscan = SkLearnCoordinateClustering(DBSCAN(eps=150, min_samples=20), minClusterSize=100)\n",
    "\n",
    "groundTruthCoordinates, groundTruthLabels = groundTruthClusters.getCoordinatesLabels()\n",
    "\n",
    "supervisedEvaluator = ClusteringModelSupervisedEvaluator(groundTruthCoordinates, trueLabels=groundTruthLabels)\n",
    "supervisedEvalStats = supervisedEvaluator.evalModel(boundedDbscan)\n",
    "\n",
    "print(\"Supervised evaluation metrics of bounded dbscan:\")\n",
    "pprint(supervisedEvalStats.getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Unsupervised Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be instructive to compare unsupervised evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "groundTruthUnsupervisedMetrics = ClusteringUnsupervisedEvalStats(groundTruthCoordinates, groundTruthLabels).metricsDict()\n",
    "boundedDbscanUnsupervisedMetrics = ClusteringUnsupervisedEvalStats.fromModel(boundedDbscan).metricsDict()\n",
    "\n",
    "pd.DataFrame({\"bounded DBSCAN\": boundedDbscanUnsupervisedMetrics, \"ground truth\": groundTruthUnsupervisedMetrics}, \n",
    "    index=groundTruthUnsupervisedMetrics.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The bounded DBSCAN is already performing quite well with the given parameters, although we see that it segregates clusters too\n",
    "much and has a general tendency towards smaller clusters. These tendencies can be seen visually by comparing the ground\n",
    "truth and the bounded DBSCAN cluster plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "groundTruthClusters.plot(markersize=0.2, cmap=\"plasma\", includeNoise=False)\n",
    "\n",
    "boundedDbscan.plot(markersize=0.2, includeNoise=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Parameter Search\n",
    "\n",
    "We can now bring everything together by running a grid search and evaluating against ground truth. Very little code\n",
    "is needed for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "parameterOptions = {\n",
    "    \"min_samples\": [19, 20, 21],\n",
    "    \"eps\": [140, 150, 160]\n",
    "}\n",
    "\n",
    "supervisedGridSearch = GridSearch(dbscanFactory, parameterOptions,\n",
    "    csvResultsPath=os.path.join(cfg.temp, \"bounded_dbscan_grid_search.csv\"))\n",
    "supervisedResultDf = supervisedGridSearch.run(supervisedEvaluator, sortColumnName=AdjustedMutualInfoScore.name,\n",
    "    ascending=False)\n",
    "supervisedResultDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "According to the adjusted mutual information score, we have now found a new parameter combination (see rightmost columns of first row) which yields results even closer to the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b3442ae4bdb9561e722e28424c33a03c16d40b3aa50369b79d367cad7b1adea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('sensai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
