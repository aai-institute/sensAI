{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to sensAI: Supervised Learning with VectorModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sensai\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging\n",
    "\n",
    "sensAI will log relevant activies and inform about ongoing processes as well as results via the log. It is therefore highly recommended that logging be enabled when using sensAI.\n",
    "\n",
    "sensAI provides a `logging` module which includes Python's standard logging module and adds some additional functionality. To enable logging, simply use its `configureLogging` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sensai.util import logging\n",
    "\n",
    "logging.configureLogging(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To additionally write log output to a file, use the function `logging.addFileLogger`.\n",
    "\n",
    "## VectorModels\n",
    "\n",
    "The central base class for supervised learning problems in sensAI is `VectorModel`. A VectorModel is any model which operates on data points that can be reprsented as vectors of data. Here, vector is to be understood not in the mathematical sense but in the computer science sense, where a vector is simply an array of (potentially arbitaririly complex) data. (The mathematical equivalent is a tuple.) Models are typically expected to be able to process more than one data point at a time and thus should be able to process a sequence of vectors. \n",
    "\n",
    "We use pandas DataFrames to represent such sequences of data points. Note that pandas DataFrames are not limited to primitive data types but can hold arbitrary objects in every cell. When dealing with a large number of inputs, DataFrames also provide at least limited meta-information in the form of column names, so we do not lose track of what is contained in which element of a data point (vector).\n",
    "\n",
    "VectorModel itself is an abstract base class, which provides a lot of useful functionality that all its specialisations inherit (as we will see later, particularly in the more advanced tutorials). The class is specialised in `VectorClassificationModel` and `VectorRegressionModel`, which in turn are specialised for various machine learning frameworks (such as sklearn and PyTorch) or can be directly subclassed to create your own model. \n",
    "\n",
    "In this tutorial, we will be dealing with a classification problem. Therefore, we will apply subclasses of `VectorClassificationModel` such as `SkLearnRandomForestVectorClassificationModel`. As an sklearn classification model which uses a well-defined training and inference interface, the implementation of the class is essentially justa few lines of code (given the intermediate abstraction `AbstractSkLearnVectorClassificationModel` for all classification models that use the sklearn protocol)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating Models\n",
    "\n",
    "First, let us load a dataset which we can experiment. sklearn provides, for example, the Iris classification dataset, where the task is to differentiate three different types of flowers based on measurements of their petals and sepals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "\n",
    "irisData = sklearn.datasets.load_iris()\n",
    "irisInputDF = pd.DataFrame(irisData[\"data\"], columns=irisData[\"feature_names\"]).reset_index(drop=True)\n",
    "irisOutputDF = pd.DataFrame({\"class\": [irisData[\"target_names\"][idx] for idx in irisData[\"target\"]]}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a sample of the data, combining both the inputs and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisCombinedDF = pd.concat((irisInputDF, irisOutputDF), axis=1)\n",
    "irisCombinedDF.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with sensAI, we typically use DataFrames such as this as the starting point.\n",
    "\n",
    "We create an instance of **InputOutputData** from the two data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisInputOutputData = sensai.InputOutputData(irisInputDF, irisOutputDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low-Level Training and Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a **DataSplitter** (see subclasses) to split the data into a training and test set, specifically a **DataSplitterFractional**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSplitter = sensai.data.DataSplitterFractional(0.8, shuffle=True)\n",
    "trainingIoData, testIoData = dataSplitter.split(irisInputOutputData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train a model. Let us train a random forest classifier, which should work well for this sort of problem. sensAI provides models from various libraries, including scikit-learn, PyTorch, lightgbm, xgboost, catboost, and TensorFlow.\n",
    "\n",
    "In this case, let us use the random forest implementation from sklearn, which is provided via the wrapper class SkLearnRandomForestVectorClassificationModel.\n",
    "\n",
    "sensAI's **VectorModel** classes (specialised for classification and regression) provide a common interface with a lot of useful functionality, which we will see later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestModel = sensai.sklearn.classification.SkLearnRandomForestVectorClassificationModel(\n",
    "    min_samples_leaf=2).withName(\"RandomForest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class suppports all the parameters supported by the original sklearn model. In this case, we only set the minimum number of samples that must end up in each leaf.\n",
    "\n",
    "We train the model using the `fitInputOutputData` method; we could also use the `fit` method, which is analogous to the sklearn interface and takes two arguments (input, output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestModel.fitInputOutputData(trainingIoData)\n",
    "randomForestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the trained model and predict the outputs for the test set we reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedOutputsDF = randomForestModel.predict(testIoData.inputs)\n",
    "predictedOutputsDF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare some of the predictions to the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat((predictedOutputsDF.rename(columns={\"class\": \"predictedClass\"}), testIoData.outputs), axis=1).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ground truth and predicted values, we could now compute the metrics we're interested in. We could, for example, use the metrics implemented in sklearn to analyse the result. Yet sensAI already provides abstractions that facilitate the generation of metrics and the collection of results. Read on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Evaluators\n",
    "\n",
    "sensAI provides evaluator abstractions which facilitate the training and evaluation of models.\n",
    "\n",
    "For a classification problem, we instantiate a VectorClassificationModelEvaluator. An evaluator serves to evaluate one or more models based on the same data, so we construct it with the data and instructions on how to handle/split the data for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluatorParams = sensai.evaluation.VectorClassificationModelEvaluatorParams(dataSplitter=dataSplitter, computeProbabilities=True)\n",
    "evaluator = sensai.evaluation.VectorClassificationModelEvaluator(irisInputOutputData, params=evaluatorParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this evaluator to evaluate one or more models. Let us evaluate the random forest model from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.fitModel(randomForestModel)\n",
    "evalData = evaluator.evalModel(randomForestModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation data holds, in particular, an **EvalStats** object, which can provide data on the quality of the results.\n",
    "Depending on the type of problem, many metrics will already be computed by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalStats = evalData.getEvalStats()\n",
    "evalStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the metrics in a dictionary as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalStats.metricsDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute additional metrics by passing a metric to the `computeMetricValue` method, but we could also have added additional metrics to the `evaluatorParams` above and have the metric included in all results.\n",
    "\n",
    "Let's see how frequently the true class is among the top two most probable classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalStats.computeMetricValue(sensai.eval_stats_classification.ClassificationMetricTopNAccuracy(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EvalStats object can also be used to generate plots, such as a confusion matrix or a precision-recall plot for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalStats.plotConfusionMatrix(normalize=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Fully-Integrated Evaluation Utilities\n",
    "\n",
    "sensAI's evaluation utilities take things one step further and assist you in out all the evaluation steps and results computations in a single call.\n",
    "\n",
    "You can perform evaluations based on a single split or cross-validation. We simply declare the necessary parameters for both types of computations (or the one type we seek to carry out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluatorParams = sensai.evaluation.VectorClassificationModelEvaluatorParams(\n",
    "    dataSplitter=dataSplitter, computeProbabilities=True, \n",
    "    additionalMetrics=[sensai.eval_stats_classification.ClassificationMetricTopNAccuracy(2)])\n",
    "crossValidatorParams = sensai.evaluation.crossval.VectorModelCrossValidatorParams(folds=10, \n",
    "    evaluatorParams=evaluatorParams)\n",
    "evalUtil = sensai.evaluation.ClassificationEvaluationUtil(irisInputOutputData, \n",
    "    evaluatorParams=evaluatorParams, crossValidatorParams=crossValidatorParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we will usually want to save evaluation results. The evaluation methods of `evalUtil` take a parameter `resultWriter` which allows us to define where results shall be written. Within this notebook, we shall simply inspect the resulting metrics in the log that is printed, and we shall configure plots to be shown directly.\n",
    "\n",
    "#### Simple Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform the same evaluation as above (which uses a single split) like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalUtil.performSimpleEvaluation(randomForestModel, showPlots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customising the Set of Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we decide that we don't really want to have the normalised confusion matrix, we can disable it for any further experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalUtil.evalStatsPlotCollector.getEnabledPlots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these are only active for binary classification. The one we don't want is \"confusion-matrix-rel\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalUtil.evalStatsPlotCollector.disablePlots(\"confusion-matrix-rel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also define our own plot class (by creating a new subclass of `ClassificationEvalStatsPlot`) and add it to the `evalStatsPlotCollector` in order to have the plot auto-generated whenever we apply one of `evalUtil`'s methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can similarly run cross-validation and produce the respective evaluation metrics with a single call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalUtil.performCrossValidation(randomForestModel, showPlots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the plot we disabled earlier is no longer being generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Models\n",
    "\n",
    "A most common use case is to compare the performance of several models. The evaluation utility makes it very simple to compare any number of models.\n",
    "\n",
    "Let's say we want to compare the random forest we have been using thus far to a simple decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evalUtil.compareModels([\n",
    "        randomForestModel, \n",
    "        sensai.sklearn.classification.SkLearnDecisionTreeVectorClassificationModel(min_samples_leaf=2).withName(\"DecisionTree\")], \n",
    "    useCrossValidation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the data frame with the aggregated metrics, which was already printed to the log, the results object contains all the data that was generated during the evaluation. We can, for example, use it to plot the distribution of one of the metrics across all the folds for one of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results.resultsDF)\n",
    "\n",
    "escRandomForest = results.resultByModelName[\"RandomForest\"].crossValData.getEvalStatsCollection()\n",
    "escRandomForest.plotDistribution(\"accuracy\", bins=np.linspace(0,1,21), stat=\"count\", kde=False);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute additional aggregations or inspect the full list of metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "escRandomForest.aggMetricsDict(aggFns=[np.max, np.min])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "escRandomForest.getValues(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generators and Data Frame Transformers\n",
    "\n",
    "When dealing with the preparation of input data for a model, we often need to cater to technical requirements of various types of models. sensAI seeks to make the process of supporting multiple input pipelines for different types of models as simple as possible - by focusing on concise, declarative semantics and integrating the model-specific data extraction and transformation mechanisms into the models themselves.\n",
    "In essence, this means:\n",
    "\n",
    "  1. **Starting with the raw or most general representation of the data**\n",
    " \n",
    "     This could mean simply starting with the data that is straightforward for us to obtain - or using directly using particular domain specific objects.\n",
    "     \n",
    "     For example, if the problem is to classify situations, we might already have a ``Situation`` class in our code which represents all the data that is is relevant to a situation (e.g. the point in time, the affected user, the location, etc.). Pandas DataFrames can represent arbitrary data, so there is no reason to not simply use as the raw input data frame that is fed to our models a single column containing instances of class ``Situation``.\n",
    "     Or we might instead directly observe a set of sensor readings, all of which are real numbers; this scenario would certainly be closer to what we typically see in machine learning data sets, but it isn't always the case in the real world.\n",
    "     \n",
    "     Whatever the case may be, we can represent it in a data frame. We call the original input data frame, which we pass to a sensAI ``VectorModel``, the *raw data frame*.\n",
    "\n",
    "  2. **Extracing features from the raw data, using their \"natural\" representation** (using ``FeatureGenerators``)\n",
    "     \n",
    "     We extract from the raw data frame pieces of information that we regard as relevant *features* for the task at hand.\n",
    "     A sensAI ``FeatureGenerator`` can generate one or more data frame columns (containing arbitrary data), and a model can be associated with any number of feature generators.\n",
    "     Several key aspects:\n",
    "\n",
    "       * FeatureGenerators crititcally decouple the original raw data from the features used by the models, enabling different models to use different sets of features or \n",
    "         entirely different representations of the same features.\n",
    "       * FeatureGenerators become part of the model and are (where necessary) jointly trained with model. This facilitates model deployment, as every sensAI model becomes a single unit\n",
    "         that can directly process raw input data, which is (usually) straightforward to supply at inference time.\n",
    "       * FeatureGenerators store meta-data on the features they generate, enabling downstream components to handle them appropriately.\n",
    "\n",
    "     The feature representation that we choose to generate can be arbitrary, but oftentimes, we will want to extract \"natural\" feature representations, which could, in priciple, be used by many types of models, albeit in different concrete forms.\n",
    "     Sequential data can be naturally represented as an array/list, categorical data can be represented using descriptive category names, and numeric data can be represented using \n",
    "     unmodified integers and floating point numbers.\n",
    "\n",
    "  3. **Transforming feature representations into a form that is suitable for the model at hand** (using ``DataFrameTransformers``)\n",
    "     \n",
    "     In the transformation stage, we address the model-specific idiosynchrasies, which may require, for example, that all features be represented as numbers (or even numbers within a limited range) or\n",
    "     that all features be discrete, that no values be missing, etc.\n",
    "     A ``DataFrameTransformer`` can, in principle perform an arbitary transformation from one data frame to another, but the typical use case is to apply transformations of feature representations that\n",
    "     are necessary for specific types of models to work (their best).\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b3442ae4bdb9561e722e28424c33a03c16d40b3aa50369b79d367cad7b1adea"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('sensai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
