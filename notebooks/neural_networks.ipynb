{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.extend([\"../src\", \"..\"])\n",
    "import sensai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import *\n",
    "import config\n",
    "\n",
    "cfg = config.get_config()\n",
    "sensai.util.logging.configureLogging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Neural networks being a very powerful class of models, especially in cases where the learning of representations from low-level information (such as pixels, audio samples or text) is key, sensAI provides many useful abstractions for dealing with this class of models, facilitating data handling, learning and evaluation.\n",
    "\n",
    "sensAI mainly provides abstractions for PyTorch, but there is also rudimentary support for TensorFlow.\n",
    "\n",
    "## Image Classification\n",
    "\n",
    "As an example use case, let us solve the classification problem of classifying digits in pixel images from the MNIST dataset. Images are greyscale (no colour information) and 28x28 pixels in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnistDF = pd.read_csv(cfg.datafile_path(\"mnist_train.csv.zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data frame contains one column for every pixel, each pixel being represented by an 8-bit integer (0 to 255)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnistDF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the I/O data for our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnistIoData = sensai.InputOutputData.fromDataFrame(mnistDF, \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the image data separated from the labels, let's write a function to restore the 2D image arrays and take a look at some of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def reshape2DImage(series):\n",
    "    return series.values.reshape(28, 28)\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(10, 5))\n",
    "for i in range(5):\n",
    "    axs[i].imshow(reshape2DImage(mnistIoData.inputs.iloc[i]), cmap=\"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Predefined Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an evaluator in order to test the performance of our models, randomly splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluatorParams = sensai.evaluation.VectorClassificationModelEvaluatorParams(fractionalSplitTestFraction=0.2)\n",
    "evalUtil = sensai.evaluation.ClassificationEvaluationUtil(mnistIoData, evaluatorParams=evaluatorParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One pre-defined model we could try is a simple multi-layer perceptron. A PyTorch-based implementation is provided via class `MultiLayerPerceptronVectorClassificationModel`. This implementation supports CUDA-accelerated computations (on Nvidia GPUs), yet we shall stick to CPU-based computation (cuda=False) in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sensai.torch\n",
    "\n",
    "nnOptimiserParams = sensai.torch.NNOptimiserParams(earlyStoppingEpochs=2, batchSize=54)\n",
    "torchMLPModel = sensai.torch.models.MultiLayerPerceptronVectorClassificationModel(hiddenDims=(50, 20), \n",
    "        cuda=False, normalisationMode=sensai.torch.NormalisationMode.MAX_ALL, \n",
    "        nnOptimiserParams=nnOptimiserParams, pDropout=0.0) \\\n",
    "    .withName(\"MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks work best on **normalised inputs**, so we have opted to apply basic normalisation by specifying a normalisation mode which will transforms inputs by dividing by the maximum value found across all columns in the training data. For more elaborate normalisation options, we could have used a data frame transformer (DFT), particularly `DFTNormalisation` or `DFTSkLearnTransformer`.\n",
    "\n",
    "sensAI's default **neural network training algorithm** is based on early stopping, which involves checking, in regular intervals, the performance of the model on a validation set (which is split from the training set) and ultimately selecting the model that performed best on the validation set. You have full control over the loss evaluation method used to select the best model (by passing a respective `NNLossEvaluator` instance to NNOptimiserParams) as well as the method that is used to split the training set into the actual training set and the validation set (by adding a `DataFrameSplitter` to the model or using a custom `TorchDataSetProvider`).\n",
    "\n",
    "Given the vectorised nature of our MNIST dataset, we can apply any type of model which can accept the numeric inputs. Let's compare the neural network we defined above against another pre-defined model, which is based on a scikit-learn implementation and uses decision trees rather than neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestModel = sensai.sklearn.classification.SkLearnRandomForestVectorClassificationModel(min_samples_leaf=1, n_estimators=10) \\\n",
    "    .withName(\"RandomForest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the two models using our evaluation utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalUtil.compareModels([randomForestModel, torchMLPModel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both models perform reasonably well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Custom CNN Model\n",
    "\n",
    "Given that this is an image recognition problem, it can be sensible to apply convolutional neural networks (CNNs), which can analyse patches of the image in order to generate more high-level features from them.\n",
    "Specifically, we shall apply a neural network model which uses multiple convolutions, a max-pooling layer and a multi-layer perceptron at the end in order to produce the classification.\n",
    "\n",
    "For classification and regression, sensAI provides the fundamental classes `TorchVectorClassificationModel` and `TorchVectorRegressionModel` respectively. Ultimately, these classes will wrap an instance of `torch.nn.Module`, the base class for neural networks in PyTorch.\n",
    "\n",
    "#### Wrapping a Custom torch.nn.Module Instance\n",
    "\n",
    "If we already had an implementation of a ``torch.nn.Module`, it can be straightforwardly adapted to a sensAI model.\n",
    "\n",
    "Let's say we had the following implementation of a torch module, which performs the steps described above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MnistCnnModule(torch.nn.Module):\n",
    "    def __init__(self, imageDim: int, outputDim: int, numConv: int, kernelSize: int, poolingKernelSize: int, \n",
    "            mlpHiddenDims: Sequence[int], outputActivationFn: sensai.torch.ActivationFunction, pDropout=0.0):\n",
    "        super().__init__()\n",
    "        k = kernelSize\n",
    "        p = poolingKernelSize\n",
    "        self.cnn = torch.nn.Conv2d(1, numConv, (k, k))\n",
    "        self.pool = torch.nn.MaxPool2d((p, p))\n",
    "        self.dropout = torch.nn.Dropout(p=pDropout)\n",
    "        reducedDim = (imageDim-k+1)/p\n",
    "        if int(reducedDim) != reducedDim:\n",
    "            raise ValueError(f\"Pooling kernel size {p} is not a divisor of post-convolution dimension {imageDim-k+1}\")\n",
    "        self.mlp = sensai.torch.models.MultiLayerPerceptron(numConv * int(reducedDim)**2, outputDim, mlpHiddenDims,\n",
    "            outputActivationFn=outputActivationFn.getTorchFunction(),\n",
    "            hidActivationFn=sensai.torch.ActivationFunction.RELU.getTorchFunction(),\n",
    "            pDropout=pDropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x.unsqueeze(1))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.dropout(x)\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this module requires 2D images as input, we will need a component that transforms the vector input that is given in our data frame into a tensor that will serve as input to the module.\n",
    "In sensAI, the abstraction for this purpose is a ``sensai.torch.Tensoriser``. A **Tensoriser** can, in principle, perform arbitrary computations in order to produce, from a data frame with N rows, one or more tensors of length N (first dimension equal to N) that will ultimately be fed to the neural network.\n",
    "\n",
    "Luckily, for the case at hand, we already have the function ``reshape2DImage`` from above to assist in the implementation of the tensoriser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageReshapingInputTensoriser(sensai.torch.RuleBasedTensoriser):\n",
    "    def _tensorise(self, df: pd.DataFrame) -> Union[torch.Tensor, List[torch.Tensor]]:\n",
    "        images = [reshape2DImage(row) for _, row in df.iterrows()]\n",
    "        return torch.tensor(np.stack(images)).float() / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we derived the class from ``RuleBasedTensorised`` rather than ``Tensoriser``, because it does not require fitting. We additionally took care of the normalisation within the tensoriser.\n",
    "\n",
    "Now we have all we need to create a sensAI ``TorchVectorClassificationModel`` that will work on the input/output data we loaded earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModule = MnistCnnModule(28, 10, 32, 5, 2, (200, 20), sensai.torch.ActivationFunction.LOG_SOFTMAX)\n",
    "nnOptimiserParams = sensai.torch.NNOptimiserParams(optimiser=sensai.torch.Optimiser.ADAMW, optimiserLR=0.01, batchSize=1024, \n",
    "    earlyStoppingEpochs=3)\n",
    "cnnModelFromModule = sensai.torch.TorchVectorClassificationModel.fromModule(\n",
    "        sensai.torch.ClassificationOutputMode.LOG_PROBABILITIES, \n",
    "        cnnModule, cuda=False, nnOptimiserParams=nnOptimiserParams) \\\n",
    "    .withInputTensoriser(ImageReshapingInputTensoriser()) \\\n",
    "    .withName(\"CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now fully defined all the necessary parameters, including parameters controlling the training of the model.\n",
    "\n",
    "We are now ready to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalUtil.performSimpleEvaluation(cnnModelFromModule);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating an Input-/Output-Adaptive Custom Model\n",
    "\n",
    "While the above approach allows us to straightforwardly encapsulate a ``torch.nn.Module``, it really doesn't follow sensAI's principle of adapting model hyperparameters based on the inputs and outputs we receive during training - whenever possible. Notice that in the above example, we had to hard-code the image dimension (``28``) as well as the number of classes (``10``), even though these parameters could have been easily determined from the data. Especially in other domains where feature engineering is possible, we might want to experiment with different combinations of features, and therefore automatically adapting to inputs is key if we want to avoid editing the model hyperparameters time and time again; similarly, we might change the set of target labels in our classification problem and the model should simply adapt to a changed output dimension.\n",
    "\n",
    "To design a model that can fully adapt to the inputs and outputs, we can simply subclass ``TorchVectorClassificationModel``, where the late instantiation of the underlying model is catered for. Naturally, delayed construction of the underlying model necessitates the use of factories and thus results in some indirections. \n",
    "\n",
    "If we had designed the above model to be within the sensAI ``VectorModel`` realm from the beginning, here's what we might have written:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CnnModel(sensai.torch.TorchVectorClassificationModel):\n",
    "    def __init__(self, cuda: bool, kernelSize: int, numConv: int, poolingKernelSize: int, mlpHiddenDims: Sequence[int], \n",
    "            nnOptimiserParams: sensai.torch.NNOptimiserParams, pDropout=0.0):\n",
    "        self.cuda = cuda\n",
    "        self.outputActivationFn = sensai.torch.ActivationFunction.LOG_SOFTMAX\n",
    "        self.kernelSize = kernelSize\n",
    "        self.numConv = numConv\n",
    "        self.poolingKernelSize = poolingKernelSize\n",
    "        self.mlpHiddenDims = mlpHiddenDims\n",
    "        self.pDropout = pDropout\n",
    "        super().__init__(sensai.torch.ClassificationOutputMode.forActivationFn(self.outputActivationFn),\n",
    "            modelClass=self.VectorTorchModel, modelArgs=[self], nnOptimiserParams=nnOptimiserParams)\n",
    "\n",
    "    class VectorTorchModel(sensai.torch.VectorTorchModel):\n",
    "        def __init__(self, parent: \"CnnModel\"):\n",
    "            super().__init__(parent.cuda)\n",
    "            self._parent = parent\n",
    "\n",
    "        def createTorchModuleForDims(self, inputDim: int, outputDim: int) -> torch.nn.Module:\n",
    "            return self.Module(int(np.sqrt(inputDim)), outputDim, self._parent)\n",
    "\n",
    "        class Module(torch.nn.Module):\n",
    "            def __init__(self, imageDim, outputDim, parent: \"CnnModel\"):\n",
    "                super().__init__()\n",
    "                k = parent.kernelSize\n",
    "                p = parent.poolingKernelSize\n",
    "                self.cnn = torch.nn.Conv2d(1, parent.numConv, (k, k))\n",
    "                self.pool = torch.nn.MaxPool2d((p, p))\n",
    "                self.dropout = torch.nn.Dropout(p=parent.pDropout)\n",
    "                reducedDim = (imageDim-k+1)/p\n",
    "                if int(reducedDim) != reducedDim:\n",
    "                    raise ValueError(f\"Pooling kernel size {p} is not a divisor of post-convolution dimension {imageDim-k+1}\")\n",
    "                self.mlp = sensai.torch.models.MultiLayerPerceptron(parent.numConv * int(reducedDim)**2, outputDim, parent.mlpHiddenDims,\n",
    "                    outputActivationFn=parent.outputActivationFn.getTorchFunction(),\n",
    "                    hidActivationFn=sensai.torch.ActivationFunction.RELU.getTorchFunction(),\n",
    "                    pDropout=parent.pDropout)\n",
    "\n",
    "            def forward(self, x):\n",
    "                x = self.cnn(x.unsqueeze(1))\n",
    "                x = self.pool(x)\n",
    "                x = x.view(x.shape[0], -1)\n",
    "                x = self.dropout(x)\n",
    "                return self.mlp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is only insignificantly more code than in the previous implementation.\n",
    "The outer class, which provides the sensAI `VectorModel` features, serves mainly to hold the parameters, and the inner class inheriting from `VectorTorchModel` serves as a factory for the `torch.nn.Module`, providing us with the input and output dimensions (number of input columns and number of classes respectively) based on the data, thus enabling the model to adapt. If we had required even more adaptiveness, we could have learnt more about the data from within the fitting process of a custom input tensoriser (i.e. we could have added an inner ``Tensoriser`` class, which could have derived further hyperparameters from the data in its implementation of the fitting method.)\n",
    "\n",
    "Let's instantiate our model and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnnModel = CnnModel(cuda=False, kernelSize=5, numConv=32, poolingKernelSize=2, mlpHiddenDims=(200,20),\n",
    "        nnOptimiserParams=nnOptimiserParams) \\\n",
    "    .withName(\"CNN'\") \\\n",
    "    .withInputTensoriser(ImageReshapingInputTensoriser())\n",
    "\n",
    "evalData = evalUtil.performSimpleEvaluation(cnnModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our CNN models does slightly improve upon the MLP model we evaluated earlier. Let's do another comparison, so we get all the metrics in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisonData = evalUtil.compareModels([torchMLPModel, cnnModelFromModule, cnnModel, randomForestModel], fitModels=False)\n",
    "comparisonData.resultsDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that any differences between the two CNN models are due only to randomness in the parameter initialisation; they are functionally identical.\n",
    "\n",
    "Could the CNN model have produced even better results? Let's take a look at some examples where the CNN model went wrong by inspecting the evaluation data that was returned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified = evalData.getMisclassifiedTriplesPredTrueInput()\n",
    "fig, axs = plt.subplots(nrows=3, ncols=3, figsize=(9,9))\n",
    "for i, (predClass, trueClass, input) in enumerate(misclassified[:9]):\n",
    "    axs[i//3][i%3].imshow(reshape2DImage(input), cmap=\"binary\")\n",
    "    axs[i//3][i%3].set_title(f\"{trueClass} misclassified as {predClass}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While some of these examples are indeed ambiguous, there still is room for improvement."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b3442ae4bdb9561e722e28424c33a03c16d40b3aa50369b79d367cad7b1adea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('sensai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
