{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.extend([\"../src\", \"..\"])\n",
    "from sensai.util import logging\n",
    "\n",
    "logging.configureLogging(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Custom Models and Feature Generators\n",
    "\n",
    "In this notebook we will demonstrate some of sensAI's main features by training a model together\n",
    "with feature extractors and custom normalization rules. This will also demonstrate how easy it is to wrap one's\n",
    "own model declaration into a sensAI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sensai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sensai as sn\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler\n",
    "from sensai import VectorRegressionModel\n",
    "from sensai.data_transformation import DFTNormalisation\n",
    "from sensai.evaluation.eval_util import createVectorModelEvaluator\n",
    "from sensai.data import InputOutputData\n",
    "from sensai.tracking.clearml_tracking import ClearMLExperiment\n",
    "import sensai.featuregen as fgen\n",
    "import matplotlib.pyplot as plt\n",
    "from config import get_config\n",
    "\n",
    "cfg = get_config(reload=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, let us load a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "housing_data = cfg.datafile_path(\"boston_housing.csv\", stage=cfg.RAW)\n",
    "housing_df = pd.read_csv(housing_data)\n",
    "\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X = housing_df.copy()\n",
    "y = pd.DataFrame({\"nox\": X.pop(\"nox\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"We will use this as target\")\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Creating a Custom Model\n",
    "\n",
    "Although sensAI provides several implementations of different models across major frameworks (SKlearn, TensorFlow,\n",
    "PyTorch), we put special care to make it easy for you to bring your own model. The `VectorModel` based\n",
    "classes provides abstractions which can be used for most learning problems of the type \"datapoint in,\n",
    "row of predictions out\". The row of predictions can contain a vector with class probabilities, one or multiple\n",
    "regression targets and so on. For problems of the type: \"datapoint in, multidimensional tensor out\", see the\n",
    "tutorial in TBA.\n",
    "\n",
    "We will use VectorModel to wrap scikit-learn's implementation of a multi layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CustomModel(VectorRegressionModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = MLPRegressor()\n",
    "\n",
    "    def _predict(self, x: pd.DataFrame) -> pd.DataFrame:\n",
    "        values = self.model.predict(x)\n",
    "        return pd.DataFrame({\"nox\": values}, index=x.index)\n",
    "\n",
    "    def _fit(self, X: pd.DataFrame, Y: pd.DataFrame):\n",
    "        self.model.fit(X, Y.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Feature Generation and Normalization\n",
    "\n",
    "Some of sensAI's core design principles include explicitness and safety of data wrangling. Special care was taken to\n",
    "ensure that everything that happens to input data during feature extraction, preprocessing, training and inference was\n",
    "intended by the user. Since for many projects feature engineering is decisive for model performance, it is absolutely\n",
    "crucial that the developer has full control over all transformations that are going on during training and inference.\n",
    "\n",
    "\n",
    "The feature generation and normalization modules helps with this, allowing fine-grained control over each step in the\n",
    "processing pipeline. Since the feature generators and the normalization data frame transforms can be bound to a sensAI\n",
    "model, it is guaranteed that the data pipeline at inference will work exactly as intended.\n",
    "If something unexpected happens at inference time, like an unknown column, wrong order of columns etc, an error will be\n",
    "raised. Errors will also be raised (unless specifically disabled) if there are columns for which no normalization rules\n",
    " have been provided for columns.\n",
    "This ensures that the user has thought about how to deal with different features and that no surprises happen.\n",
    "\n",
    "This level of control comes at the price of verbosity. sensAI classes and arguments tend to have long names,\n",
    "explaining exactly what they do and what the intended use case looks like.\n",
    "\n",
    "Below we will show an example of feature engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Defining Feature Generators\n",
    "\n",
    "Below we will define two feature generators. One will compensate the tax for fraud, by assuming that if the declared\n",
    "tax in the dataframe is above a threshold, we have to subtract some fixed value that was lied about. The threshold\n",
    "is extracted from the dataframe when the feature generator is fit.\n",
    "\n",
    "The second feature generator simply takes the columns \"crim\" and \"age\" as is and marks that they should be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TaxFraudFeaturegen(fgen.FeatureGenerator):\n",
    "    def __init__(self, tax_column=\"tax\", value_lied_about=12.0):\n",
    "        self.value_lied_about = value_lied_about\n",
    "        self.tax_column = tax_column\n",
    "        self.threshold = None\n",
    "        super().__init__(\n",
    "            normalisationRuleTemplate=DFTNormalisation.RuleTemplate(\n",
    "                transformer=MinMaxScaler()\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def _fit(self, X: pd.DataFrame, Y: pd.DataFrame, ctx=None):\n",
    "        self.threshold = X[self.tax_column].median()\n",
    "\n",
    "    def compensate_for_fraud(self, tax: float):\n",
    "        if tax > self.threshold:\n",
    "            tax = tax - self.value_lied_about\n",
    "        return tax\n",
    "\n",
    "    def _generate(self, df: pd.DataFrame, ctx=None) -> pd.DataFrame:\n",
    "        result = pd.DataFrame()\n",
    "        result[self.tax_column] = df[self.tax_column].apply(self.compensate_for_fraud)\n",
    "        return result\n",
    "\n",
    "\n",
    "crime_age_featuregen = fgen.FeatureGeneratorTakeColumns(\n",
    "    columns=[\"crim\", \"age\"],\n",
    "    normalisationRuleTemplate=DFTNormalisation.RuleTemplate(skip=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### The Feature Generator Registry\n",
    "\n",
    "We could simply take the feature generators as they are and plug them into our model but instead we demonstrate\n",
    "one more class in sensAI: the feature registry. Creating a registry is convenient for rapid experimentation\n",
    "and for keeping track of useful features in a large project. You might not know which ones will be useful for which\n",
    "model so the registry abstraction helps you checking in features into git and staying organized.\n",
    "\n",
    "Here we create the a dedicated registry for the housing features. The registry will hold factories\n",
    "of featuregens which will create singleton instances if called withing the training/inference pipeline\n",
    "(this is optional).\n",
    "The collector is pinned to a registry and allows to call the registered features by name (if desired).\n",
    "This might not make much sense in a notebook but imagine having a central feature registry somewhere in you code. This\n",
    "way you can combine the registered features with some features that you cooked up in a script, all in a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "housing_feature_registry = fgen.FeatureGeneratorRegistry(useSingletons=True)\n",
    "\n",
    "housing_feature_registry.tax = TaxFraudFeaturegen\n",
    "\n",
    "feature_collector = fgen.FeatureCollector(\"tax\", crime_age_featuregen, registry=housing_feature_registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Normalization of Input and Target\n",
    "\n",
    "Now we come to the issue of normalization. In each feature generator we have declared how the resulting\n",
    "columns should be normalized. We can use this information by instantiating `DFTNormalisation`.\n",
    "If a rule for some column is missing, the normalization object will raise an error. There is a way\n",
    "to circumvent this error - set `requireAllHandled` to False. In that case, you should probably\n",
    "use a defaultTransformerFactory to normalize all remaining columns. However, we recommend to explicitly pass\n",
    "all normalization rules to the feature generators themselves, just to be sure that nothing is missing.\n",
    "\n",
    "For normalizing the target we have to use an invertible transformer, we will take the MaxAbsScaler here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dft_normalisation = sn.data_transformation.DFTNormalisation(\n",
    "    feature_collector.getNormalizationRules(),\n",
    "    requireAllHandled=True)\n",
    "\n",
    "target_transformer = sn.data_transformation.DFTSkLearnTransformer(MaxAbsScaler())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Combining Everything with the Model\n",
    "\n",
    "Now we can plug all these components into our vector model and enjoy a safe and robust that will\n",
    "work during training and inference. The model already has methods for saving and loading and is ready to\n",
    "be deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "custom_model = CustomModel()\n",
    "\n",
    "custom_model = custom_model \\\n",
    "    .withFeatureCollector(feature_collector) \\\n",
    "    .withInputTransformers(dft_normalisation) \\\n",
    "    .withTargetTransformer(target_transformer) \\\n",
    "    .withName(\"housing_predictor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model, Tracking Results Online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the model using an evaluation util as usual, but this time we will additionally track the results online using ClearML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    clearmlExperiment = ClearMLExperiment(projectName=\"sensai_demo\", taskName=\"custom_model\")\n",
    "except:\n",
    "    # allow to run without ClearML credentials being present\n",
    "    clearmlExperiment = None\n",
    "\n",
    "evalUtil = sensai.evaluation.RegressionEvaluationUtil(InputOutputData(X, y))\n",
    "evalData = evalUtil.performSimpleEvaluation(custom_model, showPlots=True, trackedExperiment=clearmlExperiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find the URL under which the results are stored online in the log.\n",
    "\n",
    "If you missed the evaluation metrics in the log output, here they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalData.getEvalStats().metricsDict()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b3442ae4bdb9561e722e28424c33a03c16d40b3aa50369b79d367cad7b1adea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('sensai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
