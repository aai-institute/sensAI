{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.extend([\"../src\", \"..\"])\n",
    "import sensai\n",
    "import logging\n",
    "import config\n",
    "\n",
    "c = config.get_config(reload=True)\n",
    "sensai.util.logging.configureLogging(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tracking Experiments\n",
    "\n",
    "In this notebook we will demonstrate how to use sensAI's tracking utilities with evaluators\n",
    "and parameter sweeps. Several backends are supported and it is very easy to write a new custom adapter\n",
    "for a different tracking framework. In this notebook we will use [trains](https://github.com/allegroai/trains)\n",
    "as tracking backend. After running it, you can access the results on the trains\n",
    "[demoserver](https://demoapp.trains.allegro.ai/) (if you have not provided your own trains config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import geopandas as gp\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sensai.hyperopt import GridSearch\n",
    "from sensai.geoanalytics.geopandas.coordinate_clustering import SkLearnCoordinateClustering\n",
    "from sensai.evaluation.evaluator_clustering import ClusteringModelSupervisedEvaluator\n",
    "from sensai.geoanalytics.geopandas.coordinate_clustering_ground_truth import PolygonAnnotatedCoordinates\n",
    "from sensai.tracking.clearml_tracking import ClearMLExperiment\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluators\n",
    "\n",
    "The main entrypoint to reproducible experiments is the evaluator api. We will use clustering evaluation for\n",
    "demonstration purposes. We load the data and create a SupervisedClusteringEvaluator, see\n",
    "[intro to evaluation](clustering_evaluation.ipynb) for more details.\n",
    "\n",
    "[comment]: <> (TODO - use some VectorModel with an sklearn dataset instead, move the notebook to sensAI repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loading the data and ground truth labels\n",
    "sampleFile = c.datafile_path(\"sample\", stage=c.RAW) # this can point to a directory or a shp/geojson file\n",
    "sampleGeoDF = gp.read_file(sampleFile)\n",
    "groundTruthClusters = PolygonAnnotatedCoordinates(sampleGeoDF, c.datafile_path(\"sample\", stage=c.GROUND_TRUTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# creating the evaluator\n",
    "groundTruthCoordinates, groundTruthLabels = groundTruthClusters.getCoordinatesLabels()\n",
    "supervisedEvaluator = ClusteringModelSupervisedEvaluator(groundTruthCoordinates, trueLabels=groundTruthLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Setup tracking\n",
    "\n",
    "Now comes the new part - we create a tracking experiment and set it in the evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def createExperiment(projectName, taskName):\n",
    "    try:\n",
    "        return ClearMLExperiment(projectName=projectName, taskName=taskName)\n",
    "    except:  # allow to run in contexts without ClearML credentials\n",
    "        return None\n",
    "\n",
    "experiment = createExperiment(projectName=\"Demos\", taskName=\"notebook_experiment\")\n",
    "supervisedEvaluator.setTrackedExperiment(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As simple as that! Whenever we perform an evaluation, the results will be tracked. Depending on\n",
    "the backend and the particular implementation of the experiment, the code and other information\n",
    "like images will get tracked as well. We will demonstrated the tracking of the evaluation of a dbscan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "boundedDbscan = SkLearnCoordinateClustering(DBSCAN(eps=150, min_samples=20), minClusterSize=100)\n",
    "supervisedEvaluator.computeMetrics(boundedDbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plots are tracked automatically on creation.\n",
    "# Note that one should use fig.show instead of plt.show\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[6, 8])\n",
    "ax.set_title(\"Sample Ground Truth clusters\")\n",
    "groundTruthClusters.plot(includeNoise=False, markersize=0.2, cmap=\"plasma\", ax=ax)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[6, 8])\n",
    "ax.set_title(\"Predicted clusters\")\n",
    "boundedDbscan.plot(includeNoise=False, markersize=0.2, cmap=\"plasma\", ax=ax, figsize=10)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We can also add the summaries df to the experiment through explicit tracking\n",
    "\n",
    "if supervisedEvaluator.trackedExperiment:\n",
    "    logger  = supervisedEvaluator.trackedExperiment.logger\n",
    "\n",
    "    logger.report_table(title=\"Clusters Summaries\", series=\"pandas DataFrame\", iteration=0,\n",
    "                        table_plot=boundedDbscan.summaryDF().sort_values(\"numMembers\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The same mechanism works in the hyperopts module. The experiment can be set for GridSearch\n",
    "or simulated annealing. One can also set the experiment in the evaluator that is passed to\n",
    "the hyperopt objects and use that one for tracking instead. Here an example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# because of how trains works and because we are using it in jupyter, we need to manually close the existing task\n",
    "# even though the docu says, with reuse_last_task_id=False a new task would be created...\n",
    "# this step is unnecessary if one has one experiment per script execution\n",
    "# we also unset the tracked experiment in the evaluator and prepare a new one for the grid search\n",
    "\n",
    "if supervisedEvaluator.trackedExperiment:\n",
    "    supervisedEvaluator.trackedExperiment.task.close()\n",
    "    supervisedEvaluator.unsetTrackedExperiment()\n",
    "\n",
    "\n",
    "def dbscanFactory(**kwargs):\n",
    "    return SkLearnCoordinateClustering(DBSCAN(**kwargs), minClusterSize=100)\n",
    "\n",
    "parameterOptions = {\n",
    "    \"min_samples\": [10, 20],\n",
    "    \"eps\": [50, 150]\n",
    "}\n",
    "\n",
    "gridExperiment = createExperiment(projectName=\"Demos\", taskName=\"notebook_grid_search\")\n",
    "dbscanGridSearch = GridSearch(dbscanFactory, parameterOptions,\n",
    "                              csvResultsPath=os.path.join(c.temp, \"dbscanGridSearchCsv\"))\n",
    "dbscanGridSearch.setTrackedExperiment(gridExperiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "searchResults = dbscanGridSearch.run(supervisedEvaluator, sortColumnName=\"numClusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# unfortunately, the trains experiment interface is at conflict with the grid search\n",
    "# the most pragmatic solution is to simply attach the dataframe to the experiment and to use it for further evaluation\n",
    "\n",
    "if dbscanGridSearch.trackedExperiment:\n",
    "    dbscanGridSearch.trackedExperiment.logger.report_table(title=\"Results\", series=\"pandas DataFrame\", iteration=0,\n",
    "        table_plot=searchResults)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b3442ae4bdb9561e722e28424c33a03c16d40b3aa50369b79d367cad7b1adea"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('sensai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
